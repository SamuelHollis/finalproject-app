{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import logging\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the model: Ran out of input\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Llamar a la función para cargar el modelo\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m political_model, political_tokenizer, device \u001b[38;5;241m=\u001b[39m load_political_model()\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo político reentrenado (usando pickle) y el tokenizador de RoBERTa base\n",
    "def load_political_model():\n",
    "    try:\n",
    "        model_path = r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pkl\"\n",
    "        \n",
    "        # Cargar el modelo guardado con pickle\n",
    "        with open(model_path, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "        # Cargar el tokenizador de RoBERTa base\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "        # Detectar si CUDA está disponible\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        return model, tokenizer, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Llamar a la función para cargar el modelo\n",
    "political_model, political_tokenizer, device = load_political_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Inicializar el modelo político (ejemplo con RoBERTa para clasificación)\n",
    "political_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Cargar el tokenizador de RoBERTa\n",
    "political_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Detectar si CUDA está disponible y mover el modelo al dispositivo adecuado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "political_model.to(device)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "torch.save(political_model.state_dict(), r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pth\")\n",
    "\n",
    "print(\"Modelo guardado con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo del modelo existe, intentando cargarlo...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Verificar si el archivo existe\n",
    "model_path = r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pkl\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"El archivo del modelo no se encuentra en la ruta especificada.\")\n",
    "else:\n",
    "    print(\"El archivo del modelo existe, intentando cargarlo...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Después de entrenar el modelo\n",
    "torch.save(political_model.state_dict(), r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pkl\")\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\deita.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\samue\\OneDrive\\Escritorio\\oli.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "tensor([[0.9855, 0.8671, 0.0260],\n",
      "        [0.6880, 0.8762, 0.3813],\n",
      "        [0.4594, 0.0339, 0.0837]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Try a simple tensor operation to check if it's on the GPU\n",
    "x = torch.rand(3, 3)\n",
    "x = x.to(device)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! GPU Name: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\finalproject_gpu\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:05.858 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.036 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\samue\\anaconda3\\envs\\finalproject_gpu\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-09-09 19:48:06.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.553 Thread 'Thread-5': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.553 Thread 'Thread-5': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "c:\\Users\\samue\\anaconda3\\envs\\finalproject_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-09-09 19:48:07.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:07.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Descargar y cargar el modelo y tokenizador localmente\n",
    "@st.cache_resource\n",
    "def load_local_model():\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Detectar si CUDA está disponible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Cargar el modelo y moverlo a la GPU si está disponible\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Configurar el pipeline para usar GPU (device=0 para GPU)\n",
    "    return pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1), tokenizer\n",
    "\n",
    "# Cargar el modelo local\n",
    "model, tokenizer = load_local_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de etiquetas de RoBERTa a sentimientos comprensibles\n",
    "label_mapping = {\n",
    "    'LABEL_0': 'Negative',\n",
    "    'LABEL_1': 'Neutral',\n",
    "    'LABEL_2': 'Positive'\n",
    "}\n",
    "\n",
    "# Función para dividir texto en fragmentos respetando el límite de tokens\n",
    "def chunk_text(text, tokenizer, chunk_size=512):\n",
    "    tokens = tokenizer(text, truncation=True, max_length=chunk_size, return_tensors='pt')\n",
    "    input_ids = tokens.input_ids[0]\n",
    "    for i in range(0, len(input_ids), chunk_size):\n",
    "        chunk_ids = input_ids[i:i + chunk_size]\n",
    "        yield tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "\n",
    "# Función para analizar en chunks usando el modelo local y permitir la descarga de resultados como CSV\n",
    "def analyze_sentiments_chunked(df, tokenizer, chunk_size=512, process_chunk_size=5000):\n",
    "    ch_num = 0\n",
    "\n",
    "    # Inicializar la barra de progreso y el mensaje de estado\n",
    "    total_chunks = len(df) // process_chunk_size + (1 if len(df) % process_chunk_size > 0 else 0)\n",
    "    progress_bar = st.progress(0)\n",
    "    progress_text = st.empty()\n",
    "    progress_text.text(\"Analyzing...\")  # Texto que muestra el estado de análisis\n",
    "\n",
    "    # Procesar el dataframe en chunks de `process_chunk_size`\n",
    "    for start in range(0, len(df), process_chunk_size):\n",
    "        ch_num += 1\n",
    "        end = min(start + process_chunk_size, len(df))\n",
    "        chunk_df = df.iloc[start:end]\n",
    "        sentiment_list = []\n",
    "        score_list = []\n",
    "\n",
    "        for idx, text in enumerate(chunk_df['text']):\n",
    "            # Dividir en chunks\n",
    "            chunks = list(chunk_text(text, tokenizer, chunk_size=chunk_size))\n",
    "\n",
    "            # Análisis de sentimiento por chunks usando el pipeline local\n",
    "            overall_sentiment = None\n",
    "            max_score = -1  # Inicializar para que cualquier puntuación sea más alta\n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    # Usar el pipeline `sentiment_analysis` local\n",
    "                    response = sentiment_analysis(chunk)\n",
    "\n",
    "                    # Encontrar la etiqueta con la puntuación más alta\n",
    "                    for element in response:\n",
    "                        if element['score'] > max_score:\n",
    "                            max_score = element['score']\n",
    "                            overall_sentiment = element['label']\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Unexpected error: {e}\")\n",
    "                    st.error(f\"Unexpected error: {e}\")\n",
    "                    continue\n",
    "\n",
    "            sentiment_list.append(overall_sentiment)\n",
    "            score_list.append(max_score)\n",
    "\n",
    "        # Asignar los resultados al chunk procesado\n",
    "        df.loc[start:end-1, 'sentiment'] = sentiment_list\n",
    "        df.loc[start:end-1, 'score'] = score_list\n",
    "\n",
    "        # Actualizar barra de progreso\n",
    "        progress_percentage = (ch_num / total_chunks)\n",
    "        progress_bar.progress(progress_percentage)\n",
    "\n",
    "    # Completar la barra de progreso\n",
    "    progress_bar.progress(1.0)\n",
    "    progress_text.text(\"Analysis Complete!\")\n",
    "    st.success(\"Sentiment analysis complete!\")\n",
    "\n",
    "    # Convertir el DataFrame en CSV\n",
    "    csv = df.to_csv(index=False).encode('utf-8')\n",
    "\n",
    "    # Añadir botón para descargar el archivo CSV\n",
    "    st.download_button(\n",
    "        label=\"⬇️ Download results as CSV\",\n",
    "        data=csv,\n",
    "        file_name='sentiment_analysis_results.csv',\n",
    "        mime='text/csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular y mostrar los porcentajes de sentimiento\n",
    "def calculate_sentiment_percentages(df):\n",
    "    # Contar la frecuencia de cada sentimiento\n",
    "    sentiment_counts = df['sentiment'].value_counts(normalize=True) * 100\n",
    "    sentiments = ['LABEL_0', 'LABEL_1', 'LABEL_2']  # LABEL_0: Negative, LABEL_1: Neutral, LABEL_2: Positive\n",
    "    \n",
    "    # Crear una lista con los porcentajes de cada sentimiento\n",
    "    percentages = [sentiment_counts.get(sentiment, 0) for sentiment in sentiments]\n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inyección del CSS en la aplicación\n",
    "page_bg_css = '''\n",
    "<style>\n",
    "body {\n",
    "    background: url(\"https://www.omfif.org/wp-content/uploads/2024/01/GettyImages-1183053829.jpg\"); /* Background image */\n",
    "    background-size: cover;\n",
    "    background-position: cover;\n",
    "    background-repeat: no-repeat;\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    opacity: 0.7; /* Slight opacity to blend the background */\n",
    "}\n",
    "[data-testid=\"stAppViewContainer\"] {\n",
    "    background: rgba(0, 0, 0, 0.7); /* Darker overlay for better readability */\n",
    "    background-blend-mode: overlay;\n",
    "    padding: 2rem;\n",
    "    color: white; /* Ensure text is white and more visible */\n",
    "}\n",
    "h1 {\n",
    "    color: #B22222; /* Firebrick for the title */\n",
    "    font-weight: 700;\n",
    "    text-align: center;\n",
    "    margin-bottom: 15px;\n",
    "    opacity: 1;\n",
    "    background-color: rgba(255, 255, 255, 0.5); /* Semi-transparent white background */\n",
    "    padding: 4px;\n",
    "    border-radius: 10px; \n",
    "    max-width: 500px; /* Limit the width */\n",
    "    margin-left: auto; /* Center the element */\n",
    "    margin-right: auto; /* Center the element */\n",
    "}\n",
    "h2, h3 {\n",
    "    color: white; /* White text for subtitles */\n",
    "    font-weight: 700;\n",
    "    text-align: center;\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    ".stButton>button {\n",
    "    background-color: #1E90FF; /* DodgerBlue */\n",
    "    color: white;\n",
    "    font-size: 18px;\n",
    "    border-radius: 12px; /* Rounded corners */\n",
    "    padding: 10px 20px;\n",
    "    transition: all 0.3s ease;\n",
    "    box-shadow: 0 4px 10px rgba(0,0,0,0.15); /* Soft shadow */\n",
    "}\n",
    ".stButton>button:hover {\n",
    "    background-color: #1E90FF; /* Lighter blue on hover */\n",
    "    transform: scale(1.05); /* Subtle zoom effect */\n",
    "}\n",
    ".stTextArea textarea {\n",
    "    background-color: rgba(107, 107, 107, 0.9); /* More opaque gray for the text area */\n",
    "    border-radius: 12px;\n",
    "    font-size: 16px;\n",
    "    padding: 15px;\n",
    "    color: white; /* White text */\n",
    "}\n",
    "footer {\n",
    "    visibility: hidden;\n",
    "}\n",
    ".result-card {\n",
    "    background-color: rgba(107, 107, 107, 0.8); /* Más opaca */\n",
    "    border-radius: 15px;\n",
    "    padding: 20px;\n",
    "    margin-bottom: 15px;\n",
    "    box-shadow: 0 4px 10px rgba(0,0,0,0.1);\n",
    "    color: white; /* White text for the result cards */\n",
    "}\n",
    ".card-header {\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    color: #1E90FF; /* Blue header for the result card */\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    "</style>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:22.577 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:22.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:22.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:22.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Injectar el CSS en la aplicación\n",
    "st.markdown(page_bg_css, unsafe_allow_html=True)\n",
    "\n",
    "# Título de la aplicación\n",
    "st.title(\"Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.952 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.952 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Sección 1: Análisis de archivo CSV\n",
    "st.subheader(\"📂 Analyze CSV File\")\n",
    "uploaded_file = st.file_uploader(\"Upload a CSV file with a 'text' column\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Cargar el archivo CSV sin incluir el índice como columna\n",
    "    df = pd.read_csv(uploaded_file, index_col=None)\n",
    "\n",
    "    # Mostrar las primeras filas del CSV\n",
    "    st.write(\"First 5 comments from the file:\")\n",
    "    st.write(df.head())\n",
    "\n",
    "    # Botón para ejecutar el análisis de sentimientos en el CSV\n",
    "    if st.button(\"🔍 Analyze Sentiments in CSV\"):\n",
    "        if 'text' not in df.columns:\n",
    "            st.error(\"The CSV file must contain a 'text' column.\")\n",
    "        else:\n",
    "            with st.spinner(\"🔄 Analyzing sentiments, please wait...\"):\n",
    "                # Llamar a la función con todos los parámetros requeridos\n",
    "                analyzed_df = analyze_sentiments_chunked(df, tokenizer, chunk_size=512)\n",
    "\n",
    "            st.success(\"✅ Analysis complete!\")\n",
    "\n",
    "            # Display results\n",
    "            st.write(\"Analysis Results:\")\n",
    "            st.write(analyzed_df.head())\n",
    "\n",
    "            # Calculate and display sentiment percentages\n",
    "            percentages = calculate_sentiment_percentages(analyzed_df)\n",
    "            labels = ['Negative', 'Neutral', 'Positive']\n",
    "            colors = ['#FF6B6B', '#F7D794', '#4CAF50']  # Colors for negative, neutral, positive\n",
    "\n",
    "            # Create a bar chart\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.barh(labels, percentages, color=colors)\n",
    "            ax.set_xlabel('Percentage (%)')\n",
    "            ax.set_title('Sentiment Distribution')\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            # Download the results as a CSV without an index\n",
    "            csv = analyzed_df.to_csv(index=False).encode('utf-8')\n",
    "            st.download_button(\n",
    "                label=\"⬇️ Download results as CSV\",\n",
    "                data=csv,\n",
    "                file_name='sentiment_analysis_results.csv',\n",
    "                mime='text/csv',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:27.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Session state does not function when running a script without `streamlit run`\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Section 2: Individual Sentence Analysis\n",
    "st.subheader(\"📝 Analyze a Single Sentence\")\n",
    "\n",
    "# Campo para que el usuario ingrese una oración\n",
    "user_input = st.text_area(\"Write a sentence to analyze\", \"\", key=\"single_sentence_input\")\n",
    "\n",
    "if st.button(\"📊 Analyze Sentence\", key=\"analyze_sentence_button\"):\n",
    "    if user_input:  # Si el usuario ha ingresado texto\n",
    "        with st.spinner(\"🔄 Analyzing sentence...\"):\n",
    "            try:\n",
    "                # Obtener los resultados completos de cada etiqueta\n",
    "                result = sentiment_analysis(user_input)\n",
    "\n",
    "                # Crear listas para las etiquetas y las puntuaciones\n",
    "                labels = [label_mapping[res['label']] for res in result]\n",
    "                scores = [res['score'] for res in result]\n",
    "\n",
    "                # Crear un DataFrame con las etiquetas y sus probabilidades\n",
    "                sentiment_df = pd.DataFrame({\n",
    "                    'Sentiment': labels,\n",
    "                    'Probability': [score * 100 for score in scores]  # Convertir a porcentaje\n",
    "                })\n",
    "\n",
    "                # Mostrar el resultado del análisis principal\n",
    "                max_index = scores.index(max(scores))\n",
    "                sentiment = labels[max_index]\n",
    "                confidence = scores[max_index]\n",
    "\n",
    "                st.markdown(f\"\"\"\n",
    "                <div class=\"result-card\">\n",
    "                    <div class=\"card-header\">Analysis Result:</div>\n",
    "                    <p><strong>Sentiment:</strong> {sentiment}</p>\n",
    "                    <p><strong>Confidence:</strong> {confidence:.2f}%</p>\n",
    "                </div>\n",
    "                \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "                # Graficar con Seaborn\n",
    "                fig, ax = plt.subplots(figsize=(6, 4))\n",
    "                sns.barplot(x=\"Probability\", y=\"Sentiment\", data=sentiment_df, palette=\"coolwarm\", ax=ax)\n",
    "\n",
    "                # Añadir los valores sobre las barras\n",
    "                for index, value in enumerate(sentiment_df['Probability']):\n",
    "                    ax.text(value + 1, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "                # Estilo del gráfico\n",
    "                ax.set_title(\"Sentiment Probabilities\", fontsize=16, fontweight='bold')\n",
    "                ax.set_xlim(0, 100)  # Limitar el eje de las probabilidades a 100%\n",
    "                st.pyplot(fig)\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
