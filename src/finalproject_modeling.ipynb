{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import logging\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the model: Ran out of input\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Llamar a la funci√≥n para cargar el modelo\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m political_model, political_tokenizer, device \u001b[38;5;241m=\u001b[39m load_political_model()\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo pol√≠tico reentrenado (usando pickle) y el tokenizador de RoBERTa base\n",
    "def load_political_model():\n",
    "    try:\n",
    "        model_path = r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pkl\"\n",
    "        \n",
    "        # Cargar el modelo guardado con pickle\n",
    "        with open(model_path, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "        # Cargar el tokenizador de RoBERTa base\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "        # Detectar si CUDA est√° disponible\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        return model, tokenizer, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Llamar a la funci√≥n para cargar el modelo\n",
    "political_model, political_tokenizer, device = load_political_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Inicializar el modelo pol√≠tico (ejemplo con RoBERTa para clasificaci√≥n)\n",
    "political_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Cargar el tokenizador de RoBERTa\n",
    "political_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Detectar si CUDA est√° disponible y mover el modelo al dispositivo adecuado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "political_model.to(device)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "torch.save(political_model.state_dict(), r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pth\")\n",
    "\n",
    "print(\"Modelo guardado con √©xito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo del modelo existe, intentando cargarlo...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Verificar si el archivo existe\n",
    "model_path = r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pkl\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"El archivo del modelo no se encuentra en la ruta especificada.\")\n",
    "else:\n",
    "    print(\"El archivo del modelo existe, intentando cargarlo...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despu√©s de entrenar el modelo\n",
    "torch.save(political_model.state_dict(), r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado.pkl\")\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\deita.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\samue\\OneDrive\\Escritorio\\oli.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "tensor([[0.9855, 0.8671, 0.0260],\n",
      "        [0.6880, 0.8762, 0.3813],\n",
      "        [0.4594, 0.0339, 0.0837]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Try a simple tensor operation to check if it's on the GPU\n",
    "x = torch.rand(3, 3)\n",
    "x = x.to(device)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! GPU Name: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\finalproject_gpu\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:05.858 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.036 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\samue\\anaconda3\\envs\\finalproject_gpu\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-09-09 19:48:06.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.553 Thread 'Thread-5': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:06.553 Thread 'Thread-5': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "c:\\Users\\samue\\anaconda3\\envs\\finalproject_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-09-09 19:48:07.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:07.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Descargar y cargar el modelo y tokenizador localmente\n",
    "@st.cache_resource\n",
    "def load_local_model():\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Detectar si CUDA est√° disponible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Cargar el modelo y moverlo a la GPU si est√° disponible\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Configurar el pipeline para usar GPU (device=0 para GPU)\n",
    "    return pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1), tokenizer\n",
    "\n",
    "# Cargar el modelo local\n",
    "model, tokenizer = load_local_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de etiquetas de RoBERTa a sentimientos comprensibles\n",
    "label_mapping = {\n",
    "    'LABEL_0': 'Negative',\n",
    "    'LABEL_1': 'Neutral',\n",
    "    'LABEL_2': 'Positive'\n",
    "}\n",
    "\n",
    "# Funci√≥n para dividir texto en fragmentos respetando el l√≠mite de tokens\n",
    "def chunk_text(text, tokenizer, chunk_size=512):\n",
    "    tokens = tokenizer(text, truncation=True, max_length=chunk_size, return_tensors='pt')\n",
    "    input_ids = tokens.input_ids[0]\n",
    "    for i in range(0, len(input_ids), chunk_size):\n",
    "        chunk_ids = input_ids[i:i + chunk_size]\n",
    "        yield tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "\n",
    "# Funci√≥n para analizar en chunks usando el modelo local y permitir la descarga de resultados como CSV\n",
    "def analyze_sentiments_chunked(df, tokenizer, chunk_size=512, process_chunk_size=5000):\n",
    "    ch_num = 0\n",
    "\n",
    "    # Inicializar la barra de progreso y el mensaje de estado\n",
    "    total_chunks = len(df) // process_chunk_size + (1 if len(df) % process_chunk_size > 0 else 0)\n",
    "    progress_bar = st.progress(0)\n",
    "    progress_text = st.empty()\n",
    "    progress_text.text(\"Analyzing...\")  # Texto que muestra el estado de an√°lisis\n",
    "\n",
    "    # Procesar el dataframe en chunks de `process_chunk_size`\n",
    "    for start in range(0, len(df), process_chunk_size):\n",
    "        ch_num += 1\n",
    "        end = min(start + process_chunk_size, len(df))\n",
    "        chunk_df = df.iloc[start:end]\n",
    "        sentiment_list = []\n",
    "        score_list = []\n",
    "\n",
    "        for idx, text in enumerate(chunk_df['text']):\n",
    "            # Dividir en chunks\n",
    "            chunks = list(chunk_text(text, tokenizer, chunk_size=chunk_size))\n",
    "\n",
    "            # An√°lisis de sentimiento por chunks usando el pipeline local\n",
    "            overall_sentiment = None\n",
    "            max_score = -1  # Inicializar para que cualquier puntuaci√≥n sea m√°s alta\n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    # Usar el pipeline `sentiment_analysis` local\n",
    "                    response = sentiment_analysis(chunk)\n",
    "\n",
    "                    # Encontrar la etiqueta con la puntuaci√≥n m√°s alta\n",
    "                    for element in response:\n",
    "                        if element['score'] > max_score:\n",
    "                            max_score = element['score']\n",
    "                            overall_sentiment = element['label']\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Unexpected error: {e}\")\n",
    "                    st.error(f\"Unexpected error: {e}\")\n",
    "                    continue\n",
    "\n",
    "            sentiment_list.append(overall_sentiment)\n",
    "            score_list.append(max_score)\n",
    "\n",
    "        # Asignar los resultados al chunk procesado\n",
    "        df.loc[start:end-1, 'sentiment'] = sentiment_list\n",
    "        df.loc[start:end-1, 'score'] = score_list\n",
    "\n",
    "        # Actualizar barra de progreso\n",
    "        progress_percentage = (ch_num / total_chunks)\n",
    "        progress_bar.progress(progress_percentage)\n",
    "\n",
    "    # Completar la barra de progreso\n",
    "    progress_bar.progress(1.0)\n",
    "    progress_text.text(\"Analysis Complete!\")\n",
    "    st.success(\"Sentiment analysis complete!\")\n",
    "\n",
    "    # Convertir el DataFrame en CSV\n",
    "    csv = df.to_csv(index=False).encode('utf-8')\n",
    "\n",
    "    # A√±adir bot√≥n para descargar el archivo CSV\n",
    "    st.download_button(\n",
    "        label=\"‚¨áÔ∏è Download results as CSV\",\n",
    "        data=csv,\n",
    "        file_name='sentiment_analysis_results.csv',\n",
    "        mime='text/csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular y mostrar los porcentajes de sentimiento\n",
    "def calculate_sentiment_percentages(df):\n",
    "    # Contar la frecuencia de cada sentimiento\n",
    "    sentiment_counts = df['sentiment'].value_counts(normalize=True) * 100\n",
    "    sentiments = ['LABEL_0', 'LABEL_1', 'LABEL_2']  # LABEL_0: Negative, LABEL_1: Neutral, LABEL_2: Positive\n",
    "    \n",
    "    # Crear una lista con los porcentajes de cada sentimiento\n",
    "    percentages = [sentiment_counts.get(sentiment, 0) for sentiment in sentiments]\n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inyecci√≥n del CSS en la aplicaci√≥n\n",
    "page_bg_css = '''\n",
    "<style>\n",
    "body {\n",
    "    background: url(\"https://www.omfif.org/wp-content/uploads/2024/01/GettyImages-1183053829.jpg\"); /* Background image */\n",
    "    background-size: cover;\n",
    "    background-position: cover;\n",
    "    background-repeat: no-repeat;\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    opacity: 0.7; /* Slight opacity to blend the background */\n",
    "}\n",
    "[data-testid=\"stAppViewContainer\"] {\n",
    "    background: rgba(0, 0, 0, 0.7); /* Darker overlay for better readability */\n",
    "    background-blend-mode: overlay;\n",
    "    padding: 2rem;\n",
    "    color: white; /* Ensure text is white and more visible */\n",
    "}\n",
    "h1 {\n",
    "    color: #B22222; /* Firebrick for the title */\n",
    "    font-weight: 700;\n",
    "    text-align: center;\n",
    "    margin-bottom: 15px;\n",
    "    opacity: 1;\n",
    "    background-color: rgba(255, 255, 255, 0.5); /* Semi-transparent white background */\n",
    "    padding: 4px;\n",
    "    border-radius: 10px; \n",
    "    max-width: 500px; /* Limit the width */\n",
    "    margin-left: auto; /* Center the element */\n",
    "    margin-right: auto; /* Center the element */\n",
    "}\n",
    "h2, h3 {\n",
    "    color: white; /* White text for subtitles */\n",
    "    font-weight: 700;\n",
    "    text-align: center;\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    ".stButton>button {\n",
    "    background-color: #1E90FF; /* DodgerBlue */\n",
    "    color: white;\n",
    "    font-size: 18px;\n",
    "    border-radius: 12px; /* Rounded corners */\n",
    "    padding: 10px 20px;\n",
    "    transition: all 0.3s ease;\n",
    "    box-shadow: 0 4px 10px rgba(0,0,0,0.15); /* Soft shadow */\n",
    "}\n",
    ".stButton>button:hover {\n",
    "    background-color: #1E90FF; /* Lighter blue on hover */\n",
    "    transform: scale(1.05); /* Subtle zoom effect */\n",
    "}\n",
    ".stTextArea textarea {\n",
    "    background-color: rgba(107, 107, 107, 0.9); /* More opaque gray for the text area */\n",
    "    border-radius: 12px;\n",
    "    font-size: 16px;\n",
    "    padding: 15px;\n",
    "    color: white; /* White text */\n",
    "}\n",
    "footer {\n",
    "    visibility: hidden;\n",
    "}\n",
    ".result-card {\n",
    "    background-color: rgba(107, 107, 107, 0.8); /* M√°s opaca */\n",
    "    border-radius: 15px;\n",
    "    padding: 20px;\n",
    "    margin-bottom: 15px;\n",
    "    box-shadow: 0 4px 10px rgba(0,0,0,0.1);\n",
    "    color: white; /* White text for the result cards */\n",
    "}\n",
    ".card-header {\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    color: #1E90FF; /* Blue header for the result card */\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    "</style>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:22.577 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:22.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:22.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:22.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Injectar el CSS en la aplicaci√≥n\n",
    "st.markdown(page_bg_css, unsafe_allow_html=True)\n",
    "\n",
    "# T√≠tulo de la aplicaci√≥n\n",
    "st.title(\"Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.952 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:24.952 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Secci√≥n 1: An√°lisis de archivo CSV\n",
    "st.subheader(\"üìÇ Analyze CSV File\")\n",
    "uploaded_file = st.file_uploader(\"Upload a CSV file with a 'text' column\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Cargar el archivo CSV sin incluir el √≠ndice como columna\n",
    "    df = pd.read_csv(uploaded_file, index_col=None)\n",
    "\n",
    "    # Mostrar las primeras filas del CSV\n",
    "    st.write(\"First 5 comments from the file:\")\n",
    "    st.write(df.head())\n",
    "\n",
    "    # Bot√≥n para ejecutar el an√°lisis de sentimientos en el CSV\n",
    "    if st.button(\"üîç Analyze Sentiments in CSV\"):\n",
    "        if 'text' not in df.columns:\n",
    "            st.error(\"The CSV file must contain a 'text' column.\")\n",
    "        else:\n",
    "            with st.spinner(\"üîÑ Analyzing sentiments, please wait...\"):\n",
    "                # Llamar a la funci√≥n con todos los par√°metros requeridos\n",
    "                analyzed_df = analyze_sentiments_chunked(df, tokenizer, chunk_size=512)\n",
    "\n",
    "            st.success(\"‚úÖ Analysis complete!\")\n",
    "\n",
    "            # Display results\n",
    "            st.write(\"Analysis Results:\")\n",
    "            st.write(analyzed_df.head())\n",
    "\n",
    "            # Calculate and display sentiment percentages\n",
    "            percentages = calculate_sentiment_percentages(analyzed_df)\n",
    "            labels = ['Negative', 'Neutral', 'Positive']\n",
    "            colors = ['#FF6B6B', '#F7D794', '#4CAF50']  # Colors for negative, neutral, positive\n",
    "\n",
    "            # Create a bar chart\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.barh(labels, percentages, color=colors)\n",
    "            ax.set_xlabel('Percentage (%)')\n",
    "            ax.set_title('Sentiment Distribution')\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            # Download the results as a CSV without an index\n",
    "            csv = analyzed_df.to_csv(index=False).encode('utf-8')\n",
    "            st.download_button(\n",
    "                label=\"‚¨áÔ∏è Download results as CSV\",\n",
    "                data=csv,\n",
    "                file_name='sentiment_analysis_results.csv',\n",
    "                mime='text/csv',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:48:27.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Session state does not function when running a script without `streamlit run`\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-09 19:48:27.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Section 2: Individual Sentence Analysis\n",
    "st.subheader(\"üìù Analyze a Single Sentence\")\n",
    "\n",
    "# Campo para que el usuario ingrese una oraci√≥n\n",
    "user_input = st.text_area(\"Write a sentence to analyze\", \"\", key=\"single_sentence_input\")\n",
    "\n",
    "if st.button(\"üìä Analyze Sentence\", key=\"analyze_sentence_button\"):\n",
    "    if user_input:  # Si el usuario ha ingresado texto\n",
    "        with st.spinner(\"üîÑ Analyzing sentence...\"):\n",
    "            try:\n",
    "                # Obtener los resultados completos de cada etiqueta\n",
    "                result = sentiment_analysis(user_input)\n",
    "\n",
    "                # Crear listas para las etiquetas y las puntuaciones\n",
    "                labels = [label_mapping[res['label']] for res in result]\n",
    "                scores = [res['score'] for res in result]\n",
    "\n",
    "                # Crear un DataFrame con las etiquetas y sus probabilidades\n",
    "                sentiment_df = pd.DataFrame({\n",
    "                    'Sentiment': labels,\n",
    "                    'Probability': [score * 100 for score in scores]  # Convertir a porcentaje\n",
    "                })\n",
    "\n",
    "                # Mostrar el resultado del an√°lisis principal\n",
    "                max_index = scores.index(max(scores))\n",
    "                sentiment = labels[max_index]\n",
    "                confidence = scores[max_index]\n",
    "\n",
    "                st.markdown(f\"\"\"\n",
    "                <div class=\"result-card\">\n",
    "                    <div class=\"card-header\">Analysis Result:</div>\n",
    "                    <p><strong>Sentiment:</strong> {sentiment}</p>\n",
    "                    <p><strong>Confidence:</strong> {confidence:.2f}%</p>\n",
    "                </div>\n",
    "                \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "                # Graficar con Seaborn\n",
    "                fig, ax = plt.subplots(figsize=(6, 4))\n",
    "                sns.barplot(x=\"Probability\", y=\"Sentiment\", data=sentiment_df, palette=\"coolwarm\", ax=ax)\n",
    "\n",
    "                # A√±adir los valores sobre las barras\n",
    "                for index, value in enumerate(sentiment_df['Probability']):\n",
    "                    ax.text(value + 1, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "                # Estilo del gr√°fico\n",
    "                ax.set_title(\"Sentiment Probabilities\", fontsize=16, fontweight='bold')\n",
    "                ax.set_xlim(0, 100)  # Limitar el eje de las probabilidades a 100%\n",
    "                st.pyplot(fig)\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
